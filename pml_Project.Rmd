
```{r, echo=FALSE, message=FALSE, results='hide'}
#Loading libraries
library(knitr) #markdown
library(caret) #prediction
library(ggplot2) #pretty plots
library(psych) #describe() function
library(randomForest)
library(foreach) #parallel rf
library(doMC) #parallel rf
registerDoMC(cores = 4) #setting cores for parallel rf
```

```{r setoptions, echo=FALSE}
#Setting global options
```

```{r, echo=FALSE}
# Multiple plot function - 
# CREDIT: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
    require(grid)
    
    # Make a list from the ... arguments and plotlist
    plots <- c(list(...), plotlist)
    
    numPlots = length(plots)
    
    # If layout is NULL, then use 'cols' to determine layout
    if (is.null(layout)) {
        # Make the panel
        # ncol: Number of columns of plots
        # nrow: Number of rows needed, calculated from # of cols
        layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                         ncol = cols, nrow = ceiling(numPlots/cols))
    }
    
    if (numPlots==1) {
        print(plots[[1]])
        
    } else {
        # Set up the page
        grid.newpage()
        pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
        
        # Make each plot, in the correct location
        for (i in 1:numPlots) {
            # Get the i,j matrix positions of the regions that contain this subplot
            matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
            
            print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                            layout.pos.col = matchidx$col))
        }
    }
}
```

##Practical Machine Learning Course Project Writeup


###Executive Summary
In this analysis we attempt to build a machine learning model capable of predicting which of 5 possible ways a person was performing a dumbell curl exercise.  Accelerometers were placed on the belt, forearm, arm and dumbell of 6 participants and 160 measurements were taken.

A random forest was tuned to the data and was able to generate predictions with an out-of-bag error rate of 99.3%.  This was also confirmed against a validation set.  This model was then used to generate 20 predictions for the Submission portion of the Practical Machine Learning Course Project with 100% accuracy.

```{r, echo=FALSE, results='hide'}
#Downloading and reading data, if necessary

if (file.exists("pml-training.csv")){
    #skip
}else{
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",destfile="pml-training.csv",method="curl")
}

if(file.exists("pml-testing.csv")){
    #skip   
}else{
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfile="pml-testing.csv",method="curl")
    testData = read.csv("pml-testing.csv")
}
trainData = read.csv("pml-training.csv")
testData = read.csv("pml-testing.csv")
```


###Exploratory Data Analysis
The training data set contains 19,622 observations of 160 measurements.  The measurements are based on six different participants in the study. Initial investigation of the data by building plots of the "roll_belt" variable for: (a) all participants, (b) user "carlitos" and, (c) user "charles".

```{r, echo=FALSE, fig.width=9, fig.height=6, message=FALSE}
gg1 = ggplot(data=trainData,aes(seq_along(roll_belt),roll_belt)) + geom_point(aes(colour=trainData$classe)) + labs(colour="Classe",title="Roll Belt Measurement's for all Participants",y="Roll Belt Measurement",x="Index")

gg2 = ggplot(data=trainData[trainData$user_name=="carlitos",],aes(seq_along(roll_belt),roll_belt)) + geom_point(aes(colour=trainData[trainData$user_name=="carlitos",]$classe)) + labs(colour="Classe",title="Roll Belt Measurement for Carlitos",y="Roll Belt Measurement",x="Index")

gg3 = ggplot(data=trainData[trainData$user_name=="charles",],aes(seq_along(roll_belt),roll_belt)) + geom_point(aes(colour=trainData[trainData$user_name=="charles",]$classe)) + labs(colour="Classe",title="Roll Belt Measurement for Charles",y="Roll Belt Measurement",x="Index")

multiplot(gg1,gg2,gg3,cols=2)
```

The first plot on the data for all participants suggests that measurements may not have been taken in a consistent manner.  See the two groupings of measurements that are offset to eachother (vertically shifted).

The plots for "carlitos" suggest that the Roll Belt measurement may be highly predictive of classe "E".

Roll Belt appears to be able to uniquely distinguish classe "E" for charles, but in a different way than carlitos.  Also, we can see that the measurements for Roll Belt are very different when comparing charles and carlitos.  See below for means and standard deviations.

```{r, echo=FALSE}
print(paste("Roll Belt Mean for Charles = ",mean(trainData[trainData$user_name=="charles",]$roll_belt)))
print(paste("Roll Belt Mean for Carlitos = ",mean(trainData[trainData$user_name=="carlitos",]$roll_belt)))
print(paste("Roll Belt Standard Deviation for Charles = ",sd(trainData[trainData$user_name=="charles",]$roll_belt)))
print(paste("Roll Belt Standard Deviation for Carlitos = ",sd(trainData[trainData$user_name=="carlitos",]$roll_belt)))
```

We expect that this may cause model-fitting issues as measurements will be correlated with user_name and we do not want to build a model that relies on the username as a parameter (a model to predict measurements on the exact same "charles" will have limited value).  Hopefully, other measurements working in tandem with eachother will allow us to fit a model accurately.


###Data Cleaning
Both the training data and test data supplied at the course website contain 160 columns of information.

```{r, echo=FALSE}
dim(trainData)
dim(testData)
```

We will strip out the first seven columns of the data (X, user name, raw timestamp part 1, raw timestamp part 2, cvtd timestamp, new window, num window) as these do not appear to be accelerometer measurements.  These seven columns appear to be metadata inputs and a working model should only take accelerometer measurements to predict classe on future unseen datasets.  This will reduce the number of columns in our dataframe to 153.

```{r, echo=FALSE,results='hide'}
newTrainData = trainData[,-c(1:7)]
newTestData = testData[,-c(1:7)]
dim(newTrainData)
dim(newTestData)
```

In the next step, we will use the describe() function from the "psych" package to remove observations that are dominated by "NA".  These are sparse features and are unlikely to be predictive.  This will reduce the number of columns in our dataframe to 86.

```{r, echo=FALSE,results='hide'}
descTrainData = describe(newTrainData)
invalidFeatures = descTrainData[descTrainData$n<=500,][,1]
newTrainData = newTrainData[,-invalidFeatures]
newTestData = newTestData[,-invalidFeatures]
dim(newTrainData)
dim(newTestData)
```

In the final data cleaning step, we will apply the nearZeroVar() function from the "caret" package to remove features that have a near zero variance as, again, these are unlikely to be predictive.  This will reduce the number of columns in our final dataframe to 53.

```{r, echo=FALSE}
nearZeroVarFeatures = nearZeroVar(newTrainData)
newTrainData = newTrainData[,-nearZeroVarFeatures]
newTestData = newTestData[,-nearZeroVarFeatures]
dim(newTrainData)
dim(newTestData)
```


###Model Selection
We will split the training data into a "training" set and a "validation" set.  The training subset will include 80% of the observations and the remaining will be used to calculate the out-of-sample error.

```{r, echo=FALSE}
set.seed(1234)
trainingIndex = createDataPartition(newTrainData$classe, p = 0.80,list=FALSE)
training = newTrainData[trainingIndex,]
testing = newTrainData[-trainingIndex,]
```

Initially, we will fit a decision tree to gain some intuition in the structure of the data.  While easy to interpret, decision trees are known to be fairly poor predictors and we shall instead fit a random forest (an ensemble of decision treees).

Below we have fit a decision tree using the caret package.  While we are already conducting cross-validation by splitting the training data into a training/validation subsets, the caret package allows us to easily conduct 10-fold cross validation in the model training call.

```{r, echo=FALSE, fig.width=10, fig.height=7, message=FALSE}
set.seed(1234)
control = trainControl(method = "cv", number = 10)
modelFit2 = train(classe~., method = "rpart", data = training, trControl = control)
modelFit2
```

Despite a 10-fold cross validation process, the decision tree accurately predicts the training data only 50.9% of the time.  While this is better than a purely random selection of the "classe" response, it's likely that we can find a model that predicts better.

```{r, echo=FALSE, fig.width=9, fig.height=6}
plot(modelFit2$finalModel, uniform = TRUE, main = "Classification Tree")
text(modelFit2$finalModel)
```

Interestingly, the initial node of the fitted decision tree splits on the roll belt feature to predict the E classe.  We saw this relationship in our exploratory analysis. Also note that the D classe cannot be found at any of the terminal nodes so the fitted decision tree will never predict a D classe.  Hopefully we can find a leaner that can do better than that!

```{r, echo=FALSE}
confusionMatrix(testing$classe,predict(modelFit2,newdata=testing))
```

Above is a confusion matrix comparing the true responses in the 20% validation set against the predicted reposnses from the validation set.  As you can see the accuracy is approximately 49.9%.  The accuracy is lower on the validation set than the training set due to slight overfitting of the training set by the classifier.

Below we have fit a random forest using parallel cores to reduce computation time.  We have tuned the random forest by setting the "mtry" variable to either 2, 4, 8, 16 or 32.  The "mtry" variable is the number of features that are randomly chosen at each node for each bootstrapped decision tree.  If mtry is set to 4, eacho node for each decision tree that is fit to a bootstrapped sample will have be split on one of 4 randomly selected features.

```{r, echo=FALSE, fig.width=10, fig.height=7, message=FALSE}
set.seed(1234)
modelFit3 = train(classe~., method = "parRF", data = training, tuneGrid = data.frame(mtry=c(2,4,8,16,32)))
modelFit3
```

Accuracy is nearly 100% for each of the tuning parameters but mtry = 8 was the optimal value (accuracy of 99.3%).  This accuracy is an out-of-bag accuracy rate and is the accuracy rate one would expect to achieve when applying the model against an unseen test set.  

```{r, echo=FALSE, fig.width=10, fig.height=7, message=FALSE}
plot(modelFit3)
```

A plot showing the maximum accuracy for the random forest model at mtry = 8 is shown above.

```{r, echo=FALSE}
confusionMatrix(testing$classe,predict(modelFit3,newdata=testing))
```

The accuracy generated by the random forest model call (99.3%) is an out-of-bag error rate so technically an additional cross validation isn't necessary (http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr).  That said, we had already split the training set into a training/validation subset for our decision tree so to remain consistent we've used the same split for the random forest.  A confusion matrix for the validation set is shown above.  The accuracy for predictions against the validation set are approximately 99.7%.

Below we have made a function call to varImp() in the caret package.  This shows the importance of each variable.  Roll Belt and Yaw Belt are the most important variables in the dataset.

```{r, echo=FALSE, fig.width=10, fig.height=7, message=FALSE}
plot(varImp(modelFit3))
```


###Conclusion
The random forest learner is well suited to generating accurate predictions for this specific dataset.  While a single stratification of the feature set (decision tree) wasn't able to accurately predict the response, a bootstrapped ensemble of decision trees were.

Applying the random forest model to predict the 20 test cases from the Submission portion of the Course Project resulted in a 100% classification rate.  
